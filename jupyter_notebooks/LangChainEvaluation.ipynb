{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67db3f3-030b-4163-9ccc-f4cfb2ccd082",
   "metadata": {},
   "source": [
    "This notebook is  for the DeepLearning AI course on evaluation: https://learn.deeplearning.ai/langchain/lesson/6/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b44b2-4bdd-4364-9a95-8e50fe2c6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GHA_TOKEN = os.getenv('GHA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4de2ca6c-3f4a-4222-9779-cae892945985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "707d1499-2c06-4918-900c-6ab58e3dfe8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am an AI language model designed to assist with a wide range of tasks, including generating text, answering questions, providing information, and engaging in conversation. I can help with writing, research, general knowledge, and more.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm=ChatOpenAI()\n",
    "llm.predict(\"Hi, please summarise yout capabilities in 2 sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595885b-86e8-4b66-8e16-51021c9690b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/clothes.csv\"\n",
    "csv_loader = CSVLoader(file_path=file)\n",
    "docs = csv_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ea031-9a36-45a6-ba90-0ef18543353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch).from_loaders([csv_loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86faf41-46cb-4a37-b2b5-3fe24712f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm= ChatOpenAI(temperature=0.0)\n",
    "qa_search = RetrievalQA.from_chain_type(\n",
    "    llm=chat_llm, chain_type='stuff', \n",
    "    retriever=index.vectorstore.as_retriever(),\n",
    "    verbose=True,\n",
    "    chain_type_kwargs = {\n",
    "        'document_separator':'<<<<>>>>'\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb15b1f-3474-448a-be57-c67476a47c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "\n",
    "## here we generate questions and answers from individual documents\n",
    "## to create 'gold standard' answers\n",
    "chain = QAGenerateChain.from_llm(chat_llm)\n",
    "examples = chain.apply_and_parse([{'doc':t} for t in docs[0:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7002b-3e00-45ce-8c98-31c03acd7600",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f7641-4fb9-45d2-960b-6fef1a681e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "## how to turn on debugging to see what happens on each stage\n",
    "langchain.debug=False\n",
    "qa_search.run(examples[1]['qa_pairs']['query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b1d76-ad2b-429d-8a72-c0541c294308",
   "metadata": {},
   "source": [
    "Now we run the same questions as we asked of individual docs and compare it on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37ff4d-2910-439f-aac4-4df7e3a5801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = qa_search.run([k['qa_pairs'] for k in   examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fafd78e-f60f-4698-9ea0-cb776ed3ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e47e0-60a8-49e9-927b-10cb777f8076",
   "metadata": {},
   "source": [
    "Now we evaluate the test and real answers and see how similar they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2e792-7b4a-4bd7-99f8-f6b5864526e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(chat_llm)\n",
    "result = eval_chain.evaluate([k['qa_pairs'] for k in   examples], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad0299-9c95-4a7e-8295-1553fed0d8ac",
   "metadata": {},
   "source": [
    "Now we print out the test Q and A, real A and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1231f-9c41-4ab1-afe5-c4eae624955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in enumerate(examples):\n",
    "    print (f\"Test question: {examples[i]['qa_pairs']['query']}\")\n",
    "    print (f\"Test answer: {examples[i]['qa_pairs']['answer']}\")\n",
    "    print (f\"Real answer: {predictions[i]['result']}\")\n",
    "    print (f\"Grade: {result[i]['results']}\")\n",
    "    print(\"-----------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
