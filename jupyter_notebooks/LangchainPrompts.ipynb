{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ecfc3e-8683-46f8-a881-f61776adaedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "This notebook is from the https://learn.deeplearning.ai/langchain/lesson/3/memory and Prompt workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3821159e-7b71-40a8-b2b2-302396aa7753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.github import GitHubIssuesLoader\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GHA_TOKEN = os.getenv('GHA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbddee2-a6cb-4ff0-9fbf-7763909fae66",
   "metadata": {},
   "source": [
    "This section how to use prompt templates and output parser to get json info from a query\n",
    "\n",
    "Note that StructuredOutputParser is only good for simple key/value pairs. Doesn't handle arrays or complex objects; \n",
    "for that you'd need Pydantic or Function parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0859a-c2da-45a6-b49a-b92416b7b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers  import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "templateS = \"\"\"\n",
    " Suggest a {style} product name  and tagline  for a company making {product}. {format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "schema3 = ResponseSchema(name=\"name\", description=\"\"\"The name of the product\n",
    "\"\"\", type=\"string\")\n",
    "schema4 = ResponseSchema(name=\"name\", description=\"A catchy tagline for the product\", type=\"string\")\n",
    "schemas = [schema3, schema4]\n",
    "parser = StructuredOutputParser(response_schemas=schemas, only_json=True)\n",
    "\n",
    "## we can set partial vars with the formatting instructions\n",
    "prompt = ChatPromptTemplate.from_template(templateS, output_parser=parser, \n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
    "llm = ChatOpenAI( model='gpt-4',temperature=0.5)\n",
    "\n",
    "message = prompt.format_messages(style='serious', product='car')\n",
    "resp=llm(message)\n",
    "\n",
    "parser.parse(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25eaad2-5bf8-4830-8352-c696c55d6c83",
   "metadata": {},
   "source": [
    "This next section shows 4 sorts of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f6125c44-00a6-4f07-869c-338e68dc0afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Richard.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi I'm Richard\\nAI: hello from OpenaI\\nHuman: How are you\\nAI: Just hanging\\nHuman: whats four squared\\nAI: 16\\nHuman: What's my name again?\\nAI: Your name is Richard.\"}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this appends to existing memory with each conversation\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory =ConversationBufferMemory()\n",
    "## add 3 messages,  these are all remembered\n",
    "memory.save_context(inputs={\"input\":  \"Hi I'm Richard\"}, outputs={\"output\":\"hello from OpenaI\"})\n",
    "memory.save_context(inputs={\"input\":  \"How are you\"}, outputs={\"output\":\"Just hanging\"})\n",
    "memory.save_context(inputs={\"input\":  \"whats four squared\"}, outputs={\"output\":\"16\"})\n",
    "chain = ConversationChain(memory = memory, llm=llm)\n",
    "resp = chain.predict(input =\"What's my name again?\")\n",
    "print(resp)\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e475124e-7d23-46c4-9e24-7002e931a6ad",
   "metadata": {},
   "source": [
    "Now let's use window of size 2. It'll forget my na,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "725ed8c5-b954-4328-98ab-cc48599f3dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but as an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: whats four squared\\nAI: 16\\nHuman: What's my name again?\\nAI: I'm sorry, but as an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\"}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## this remembers up to last k exchanges\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory =ConversationBufferWindowMemory(k=2) # k = size of window\n",
    "## add 3 messages, the 1st onw will be forgotten\n",
    "memory.save_context(inputs={\"input\":  \"Hi I'm Richard\"}, outputs={\"output\":\"hello from OpenaI\"})\n",
    "memory.save_context(inputs={\"input\":  \"How are you\"}, outputs={\"output\":\"Just hanging\"})\n",
    "memory.save_context(inputs={\"input\":  \"whats four squared\"}, outputs={\"output\":\"16\"})\n",
    "chain = ConversationChain(memory = memory, llm=llm)\n",
    "resp = chain.predict(input =\"What's my name again?\")\n",
    "print(resp)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "09c9a3c3-4633-4145-8407-ef2a6f397739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: What's four squared\\nAI: 16\\nHuman: what's the answer again?\\nAI: The answer is 16.\"}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3f2ccf5b-c9bf-4361-b2b6-6bab33694612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Richard.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': \"System: Richard introduces himself to the AI from OpenAI and asks about his agenda for the day. The AI informs him that he has a dentist appointment at 4pm, followed by a meeting in London.\\nHuman: How are you\\nAI: Just hanging\\nHuman: whats four squared\\nAI: 16\\nHuman: What's my name again?\\nAI: Your name is Richard.\"}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## this summarises the conversation history once the max size has been reached\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory =ConversationSummaryBufferMemory(llm=llm,max_token_limit=50) # k = size of window\n",
    "## add 3 messages, the 1st onw will be forgotten\n",
    "memory.save_context(inputs={\"input\":  \"Hi I'm Richard\"}, outputs={\"output\":\"hello from OpenaI\"})\n",
    "memory.save_context(inputs={\"input\":  \"What's my agenda today\"}, outputs={\"output\":\"You are going to the dentist at 4pm, then are off to London for the meeting\"})\n",
    "memory.save_context(inputs={\"input\":  \"How are you\"}, outputs={\"output\":\"Just hanging\"})\n",
    "memory.save_context(inputs={\"input\":  \"whats four squared\"}, outputs={\"output\":\"16\"})\n",
    "chain = ConversationChain(memory = memory, llm=llm)\n",
    "resp = chain.predict(input =\"What's my name again?\")\n",
    "print(resp)\n",
    "memory.load_memory_variables({})## the system role includes a summary of the conversation thus far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8b152db2-7254-4b8c-ba36-afdd31ddc669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Deven': '', 'Sam': '', 'Hackathon': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: What's my agenda today\n",
      "AI: You are going to the dentist at 4pm, then are off to London for the meeting\n",
      "Human: How are you\n",
      "AI: Just hanging around in Edinburgh\n",
      "Human: whats four squared\n",
      "AI: 16\n",
      "Last line:\n",
      "Human: Deven and Sam are working on a hackathon project\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "That sounds exciting! Do you know what the project is about or what problem they are trying to solve in the hackathon?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Deven', 'Sam', 'Hackathon']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this summarises the conversation history once the max size has been reached\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "\n",
    "memory =ConversationEntityMemory(llm=llm)\n",
    "\n",
    "memory.save_context(inputs={\"input\":  \"Hi I'm Richard, a developer\"}, outputs={\"output\":\"hello from OpenaI\"})\n",
    "memory.save_context(inputs={\"input\":  \"What's my agenda today\"}, outputs={\"output\":\"You are going to the dentist at 4pm, then are off to London for the meeting\"})\n",
    "memory.save_context(inputs={\"input\":  \"How are you\"}, outputs={\"output\":\"Just hanging around in Edinburgh\"})\n",
    "memory.save_context(inputs={\"input\":  \"whats four squared\"}, outputs={\"output\":\"16\"})\n",
    "chain = ConversationChain(memory = memory, llm=llm, prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,verbose=True)\n",
    "resp = chain.predict(input=\"Deven and Sam are working on a hackathon project\")\n",
    "print(resp)\n",
    "memory.entity_cache# the system role includes a summary of the conversation thus far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cfa96090-545e-456c-94be-06bc2f59587e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Github': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: How are you\n",
      "AI: Just hanging around in Edinburgh\n",
      "Human: whats four squared\n",
      "AI: 16\n",
      "Human: Deven and Sam are working on a hackathon project\n",
      "AI: That sounds exciting! Do you know what the project is about or what problem they are trying to solve in the hackathon?\n",
      "Last line:\n",
      "Human: they are working on an alternative to Github\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's an ambitious project! Building an alternative to Github would involve creating a platform for version control and collaboration, which is quite complex. It could include features like issue tracking, pull requests, code reviewing, and more. Do they have any specific features or improvements in mind that they want to implement in their alternative?\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"they are working on an alternative to Github\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "de6c0d2b-085b-4ccd-bc0d-ed13500eb855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Deven': 'Deven is currently working on a hackathon project with Sam.',\n",
       " 'Sam': 'Sam is currently working on a hackathon project with Deven.',\n",
       " 'Hackathon': 'A hackathon is an event where Deven and Sam are currently working on a project.',\n",
       " 'Github': 'Github is a platform for version control and collaboration, featuring issue tracking, pull requests, and code reviewing.'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.entity_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0264bafe-8adf-4280-b548-73437f178f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37442fd6-c53c-41bb-a147-c56335316a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9305b2-4470-4ce5-9ee3-83b162809801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6603e-b95b-415f-a641-6cbad24b416e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca56c61-904b-4787-bd73-0894a05f36e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
