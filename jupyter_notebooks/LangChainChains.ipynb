{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a4012b-3e1e-4dc3-bb57-cb02c6bdbfb4",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae0c26-4387-4b93-8391-861287dee846",
   "metadata": {},
   "source": [
    "This is based on https://learn.deeplearning.ai/langchain/lesson/4/chains lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2554c3-bb64-4f26-93bd-79d80213f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.github import GitHubIssuesLoader\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GHA_TOKEN = os.getenv('GHA')\n",
    "os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84269f-94b0-4b23-99ba-0c2316fc87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "chat_llm = ChatOpenAI(temperature=0.8)\n",
    "memory =ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f73c7-4553-4785-b1a0-2a28ece66c4d",
   "metadata": {},
   "source": [
    "LLM chain is one of the simplest and just connects a LLM and Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be2290-d3b1-4ddd-a273-02494a539613",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "\n",
    "{\"product\":\"Apple Mac\", \"review\": \"really excellent and fast\"},\n",
    "{\"product\":\"Windward isles bananas\", \"review\": \"fresh and tasty\"},\n",
    "{\"product\":\"Cat scarer\", \"review\": \"Didn't work, the cats paid it no attention\"},\n",
    "{\"product\":\"Dog collar\", \"review\": \"Our dog liked it but slipped his collar and escaped\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c462a0ae-cb28-40af-a6dd-a3d2f4acd34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "getCompanyNameT = PromptTemplate.from_template(\"For a product {product} think of a good company name in a {style} style\")\n",
    "chain = LLMChain(llm=chat_llm, prompt=getCompanyNameT,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f9f3c-9cbb-4252-bc68-dfe55224bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(product=data[1]['product'], style=\"jokey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e132f-5c18-470c-8918-08576a175b15",
   "metadata": {},
   "source": [
    "The next is a simple sequential chain where we can put the output of 1 message into the input of the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f377de02-ab60-44af-a192-310352f23f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "## only works with single input and output\n",
    "getCompanyNameT = PromptTemplate.from_template(\"For a product {product} think of a good company name in comedy style\")\n",
    "getCompanyTaglineT = PromptTemplate.from_template(\"For a companyName {companyName} think of a good tagline\")\n",
    "## we don't need to worry about input/output names as there is only 1\n",
    "chain1 = LLMChain(llm=chat_llm, prompt=getCompanyNameT,verbose=True)\n",
    "chain2 = LLMChain(llm=chat_llm, prompt=getCompanyTaglineT,verbose=True)\n",
    "\n",
    "sequential_chain = SimpleSequentialChain(chains=[chain1, chain2])\n",
    "resp=sequential_chain.run(data[0]['product'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f2101-fe2b-4d7c-91e2-454b4887210e",
   "metadata": {},
   "source": [
    "Question - how to get intermediate outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd43fb6b-e7c9-4782-b8be-ec01b1c3fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb69b51-d88b-404e-9c5a-b4ef93bf2cc6",
   "metadata": {},
   "source": [
    "Now let's use a regular Sequential Chain to use multiple inputs and outputs.\n",
    "We need to define the output keys of each subchain\n",
    "\n",
    "For the sequential chain wrapper we need to define the first 2 inputs and final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a2ac0a-c1a0-4901-a607-61bc98e6bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "getCompanyNameT = PromptTemplate.from_template(\"For a product {product} think of a good company name in {style} style\")\n",
    "getCompanyTaglineT = PromptTemplate.from_template(\"For a companyName {companyName} think of a good tagline\")\n",
    "sentimentT = PromptTemplate.from_template(\"for a tagline {tagline} return the sentiment which must be one of Positive, Neutral or Negative\")\n",
    "summaryT = PromptTemplate.from_template(\"Summarise whether {companyName} with tagline {tagline} and sentiment {sentiment} is a good name\")\n",
    "\n",
    "llm_chain1 = LLMChain(llm=chat_llm, prompt=getCompanyNameT,verbose=True,output_key=\"companyName\")\n",
    "llm_chain2 = LLMChain(llm=chat_llm, prompt=getCompanyTaglineT,verbose=True, output_key=\"tagline\")\n",
    "llm_chain3 = LLMChain(llm=chat_llm, prompt=sentimentT,verbose=True, output_key=\"sentiment\")\n",
    "llm_chain4 = LLMChain(llm=chat_llm, prompt=summaryT,verbose=True, output_key=\"summary\")\n",
    "sub_chains = [llm_chain1,llm_chain2,llm_chain3,llm_chain4]\n",
    "seq_chain = SequentialChain(chains=sub_chains, input_variables=[\"product\", \"style\"],output_variables=[\"summary\"] )\n",
    "seq_chain.run(product=data[3]['product'], style=\"silly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af14770-9be3-466c-9ce8-4f23b9929351",
   "metadata": {},
   "source": [
    "Now we'll use router chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f319c5c-3505-4e7c-a96b-895c5433a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create some prompts \n",
    "physics_promptT = \"\"\"\n",
    " You are highly knowledgable about physics and are able to give detailed answers  about physics.\n",
    " Here is a question: {input}\n",
    " \"\"\"\n",
    "english_promptT = \"\"\"\n",
    " You are highly knowledgable about english literature and have read many classic texts.\n",
    "  You  are able to give detailed answers  about english novels.\n",
    " Here is a question: {input}\n",
    " \"\"\"\n",
    "spanish_promptT = \"\"\"\n",
    " You are highly knowledgable about spanish grammar and language and are able to give detailed explanations\n",
    " based on your years of experience conversing in Spanish.\n",
    " Here is a question: {input}\n",
    " \"\"\"\n",
    "generic_promptT = \"\"\"\n",
    " Here is a question: {input}\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6977a30-067e-4f7b-816d-5b739c2d749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is used to create the router prompt\n",
    "prompt_info = [\n",
    " {\n",
    "    \"name\":\"physics_prompt\",\n",
    "     \"description\" : \"good  for answering questions about physics\",\n",
    "     \"prompt_template\":physics_promptT\n",
    " },\n",
    "{\n",
    "    \"name\":\"english_prompt\",\n",
    "     \"description\" : \"good  for answering questions about english literature\",\n",
    "     \"prompt_template\":english_promptT\n",
    " },  \n",
    " {\n",
    "    \"name\":\"spanish_prompt\",\n",
    "     \"description\" : \"good  for answering questions about spanish\",\n",
    "     \"prompt_template\":spanish_promptT\n",
    " }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfea2fc-6823-48c5-870c-5b895f2ae56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router import multi_prompt_prompt\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "\n",
    "destination_chains = {}\n",
    "for prompt in prompt_info:\n",
    "    name = prompt['name']\n",
    "    templateS = prompt['prompt_template']\n",
    "    promptT = ChatPromptTemplate.from_template(templateS)\n",
    "    llm_chain = LLMChain(llm=chat_llm, prompt=promptT)\n",
    "    destination_chains[name]=llm_chain\n",
    "default_chain = LLMChain(llm=chat_llm, prompt=ChatPromptTemplate.from_template(\"{{input}}\"))\n",
    "\n",
    "## This goes into the router template as a choice of routers to use\n",
    "destinations = [f\"{p['name']}:   {p['description']}\" for p in prompt_info]\n",
    "destination_string= '\\n'.join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228642e8-47d5-486b-967c-c17ef764258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## partially fill in the main template\n",
    "router_templateS = multi_prompt_prompt.MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destination_string)\n",
    "\n",
    "## now create the router prompt and add it to router chain\n",
    "router_prompt = PromptTemplate(template=router_templateS, input_variables=[\"input\"], output_parser=RouterOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6453ba-6362-4550-95ad-5b68df458663",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_chain = LLMRouterChain.from_llm(chat_llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ade909-0766-4c50-aeb7-a3e165bd0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## this is the final chain.\n",
    "top_chain = MultiPromptChain(destination_chains=destination_chains, verbose=True,router_chain=router_chain,default_chain=default_chain)\n",
    "top_chain.run(\"tell me about jane austen's book northanger abbey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4fc3e-7f64-44f3-84d8-cc85539b8777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
